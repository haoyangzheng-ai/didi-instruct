# Progressive distillation on top of MDLM (SUBS)
name: progress_distill

# ---- must match the trained MDLM (teacher/student must be identical here) ----
backbone: dit                  # dit / dimamba / hf_dit
parameterization: subs
time_conditioning: False
subs_masking: False
causal_attention: False
ignore_bos: False

gumbel_tau_log10_start: -1
gumbel_tau_log10_end: -1
curriculum_start: -1
curriculum_end: -1

# Continuous-time (same regime as MDLM)
T: 0                           # 0 = continuous time

# Distillation loss (student || teacher) on MASK positions
loss_type: kl-bwd              # kl-fwd / kl-bwd / mse / tvd
teacher_ema: false
print_para: false
output_dir: "/scratch/gautschi/zheng528/duo/outputs"
print_every: 0

# ---- progressive distillation knobs ----
num_distill_steps: 2           # K: teacher rolls K reverse steps to build targets
grow_dt_every: 10000           # increase dt every N global steps (dt *= 2)
min_num_sampl_steps: 1         # stop when effective steps < this threshold
use_ema_on_growth: true        # copy EMA weights to teacher at growth
reset_opt_on_growth: false     # optionally reset optimizer/scheduler at growth

# ---- placeholders kept for template parity (not used by progressive) ----
integral_cache_path: ${hydra:runtime.cwd}/integral/${data.tokenizer_name_or_path}.pkl
posterior_loss_weight: 0.0